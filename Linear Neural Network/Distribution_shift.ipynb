{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60adb7de",
   "metadata": {},
   "source": [
    "## COVARIATE SHIFT PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb53123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.04899999871850014\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generate_data(n, mean, p=2):\n",
    "    \"\"\"\n",
    "    mean: shift of the feature distribution\n",
    "    p: exponent used for label rule (same for train & test)\n",
    "    \"\"\"\n",
    "    X = torch.randn(n, 2) + torch.tensor(mean, dtype=torch.float)\n",
    "    y = (X[:, 0]**p + X[:, 1]**p > 0).long()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = generate_data(5000, mean=[-2, -2], p=2)\n",
    "\n",
    "X_test, y_test = generate_data(2000, mean=[+2, +2], p=2)\n",
    "\n",
    "\n",
    "model = nn.Linear(2, 2)  # 2 inputs → 2 classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X_train)\n",
    "    loss = criterion(out, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def accuracy(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        preds = model(X).argmax(dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "train_acc = accuracy(model, X_train, y_train)\n",
    "test_acc = accuracy(model, X_test, y_test)\n",
    "\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# we can tell that training and testing accuracy doesn't match causing covariance shift problem.\n",
    "# what can be done ??????\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c22cd",
   "metadata": {},
   "source": [
    "SOLUTION \n",
    "\n",
    "\n",
    "1) We need to build a classifier (Logistic classifier model)\n",
    "2) It tries to segregate the test and train datapoints \n",
    "3) we assign high weightage to the datapoint which is similar to test data rather than the train datapoint \n",
    "4) Finally based on this we update the weights of the model via training and fix this issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcf0b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = [-2.0, -2.0]\n",
    "cov_train  = [[1.0, 0.0],\n",
    "              [0.0, 1.0]]\n",
    "\n",
    "mean_test = [2.0, 2.0]\n",
    "cov_test  = [[1.0, 0.0],\n",
    "             [0.0, 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1f7d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Logistic_Regression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 2)  # 2 inputs →\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "model_unweighted = Logistic_Regression()\n",
    "model_weighted   = Logistic_Regression()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")   # important: no reduction\n",
    "optimizer_unw = optim.Adam(model_unweighted.parameters(), lr=0.05)\n",
    "optimizer_w   = optim.Adam(model_weighted.parameters(), lr=0.05)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08590315",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist = torch.distributions.MultivariateNormal(\n",
    "    torch.tensor(mean_train, dtype=torch.float),\n",
    "    torch.tensor(cov_train, dtype=torch.float)\n",
    ")\n",
    "test_dist = torch.distributions.MultivariateNormal(\n",
    "    torch.tensor(mean_test, dtype=torch.float),\n",
    "    torch.tensor(cov_test, dtype=torch.float)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.exp(test_dist.log_prob(X_train) - train_dist.log_prob(X_train))\n",
    "\n",
    "w = w / w.mean() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20d6da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, weighted=False):\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        \n",
    "        if weighted:\n",
    "            loss = (w * loss).mean()   # apply importance weights\n",
    "        else:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "train(model_unweighted, optimizer_unw, weighted=False)\n",
    "train(model_weighted, optimizer_w, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (no weights): 1.0\n",
      "Test Accuracy  (no weights): 0.0794999971985817\n",
      "\n",
      "Train Accuracy (weighted): 0.510200023651123\n",
      "Test Accuracy  (weighted): 1.0\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        preds = model(X).argmax(dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "\n",
    "print(\"Train Accuracy (no weights):\", accuracy(model_unweighted, X_train, y_train))\n",
    "print(\"Test Accuracy  (no weights):\", accuracy(model_unweighted, X_test, y_test))\n",
    "print()\n",
    "print(\"Train Accuracy (weighted):\", accuracy(model_weighted, X_train, y_train))\n",
    "print(\"Test Accuracy  (weighted):\", accuracy(model_weighted, X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "# Model fixes the training data by adding weightage to datapoints similar to test data and train data is updated and tested \n",
    "# So Covariate shift problem is solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837f3e6",
   "metadata": {},
   "source": [
    "### LABEL SHIFT CORRECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cc294a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_data_label_shift(n, mean):\n",
    "    X = torch.randn(n, 2) + torch.tensor(mean)\n",
    "    logits = X[:,0] + X[:,1]\n",
    "    y = (logits > 0).long()\n",
    "    return X, y\n",
    "\n",
    "# Train distribution\n",
    "X_train, y_train = generate_data_label_shift(5000, mean=[0,0])\n",
    "\n",
    "# Test distribution with changed label probability\n",
    "X_test, y_test = generate_data_label_shift(5000, mean=[0,0])\n",
    "\n",
    "# Introduce label shift artificially\n",
    "# Flip 70% of negatives to positives\n",
    "mask = (y_test == 0)\n",
    "flip = torch.rand(mask.sum()) < 0.7\n",
    "y_test_shifted = y_test.clone()\n",
    "y_test_shifted[mask] = flip.long()\n",
    "\n",
    "# Validation distribution (clean, no shift)\n",
    "X_val, y_val = generate_data_label_shift(2000, mean=[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b6176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Train Accuracy: 0.9660000205039978\n",
      "Baseline Test Accuracy: 0.6489999890327454\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    opt.zero_grad()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_train = model(X_train).argmax(1)\n",
    "    pred_test = model(X_test).argmax(1)\n",
    "\n",
    "baseline_train_acc = (pred_train == y_train).float().mean().item()\n",
    "baseline_test_acc = (pred_test == y_test_shifted).float().mean().item()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Baseline Train Accuracy:\", baseline_train_acc)\n",
    "print(\"Baseline Test Accuracy:\", baseline_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3e6dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = generate_data_label_shift(2000, mean=[0,0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_pred = model(X_val).argmax(dim=1)\n",
    "\n",
    "# Confusion matrix C[i,j] = P(pred=i | true=j)\n",
    "C = torch.zeros(2,2)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        C[i,j] = ((val_pred == i) & (y_val == j)).sum()\n",
    "\n",
    "# Normalize columns so each column sums to 1\n",
    "C = C / C.sum(dim=0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "612d4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred_test = model(X_test).argmax(dim=1)\n",
    "\n",
    "q = torch.stack([(pred_test==0).float().mean(),\n",
    "                 (pred_test==1).float().mean()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d11db8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred_test = model(X_test).argmax(dim=1)\n",
    "\n",
    "q = torch.stack([(pred_test==0).float().mean(),\n",
    "                 (pred_test==1).float().mean()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85df33de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9878, 1.0124])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = torch.linalg.solve(C, q)\n",
    "p_test = torch.clamp(p_test, min=1e-6)\n",
    "p_test = p_test / p_test.sum()\n",
    "\n",
    "\n",
    "p_train = torch.tensor([\n",
    "    (y_train==0).float().mean(),\n",
    "    (y_train==1).float().mean()\n",
    "])\n",
    "\n",
    "w = p_test / p_train\n",
    "w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca16eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shift = nn.Sequential(nn.Linear(2, 2))\n",
    "opt = optim.Adam(model_shift.parameters(), lr=0.01)\n",
    "loss_fn_reduced = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "for epoch in range(200):\n",
    "    opt.zero_grad()\n",
    "    logits = model_shift(X_train)\n",
    "    sample_weights = w[y_train]\n",
    "    loss = (sample_weights * loss_fn_reduced(logits, y_train)).mean()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6f412fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred_test_corrected = model_shift(X_test).argmax(1)\n",
    "\n",
    "corrected_test_acc = (pred_test_corrected == y_test_shifted).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26d3a9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      " BASELINE (No Label-Shift Correction)\n",
      " Train Accuracy          : 0.966\n",
      " Test Accuracy (shifted) : 0.649\n",
      "----------------------------------------------\n",
      " ESTIMATED test label distribution p_test: tensor([0.4963, 0.5037])\n",
      " Train label distribution p_train        : tensor([0.5024, 0.4976])\n",
      " Importance weights                      : tensor([0.9878, 1.0124])\n",
      "----------------------------------------------\n",
      " AFTER LABEL-SHIFT CORRECTION (BBSE)\n",
      " Corrected Test Accuracy : 0.6534\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============================================\")\n",
    "print(\" BASELINE (No Label-Shift Correction)\")\n",
    "print(\" Train Accuracy          :\", round(baseline_train_acc, 4))\n",
    "print(\" Test Accuracy (shifted) :\", round(baseline_test_acc, 4))\n",
    "print(\"----------------------------------------------\")\n",
    "print(\" ESTIMATED test label distribution p_test:\", p_test)\n",
    "print(\" Train label distribution p_train        :\", p_train)\n",
    "print(\" Importance weights                      :\", w)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\" AFTER LABEL-SHIFT CORRECTION (BBSE)\")\n",
    "print(\" Corrected Test Accuracy :\", round(corrected_test_acc, 4))\n",
    "print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no much difference in test accuracy after label shift correction because the model was already performing well on the shifted data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

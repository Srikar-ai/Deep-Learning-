{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b008dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import cross_entropy \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "091f3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Classification:\n",
    "\n",
    "    # initialize weights and biases\n",
    "    def __init__(self,input_dim,output_dim, device='cpu', learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "        self.W = torch.randn(input_dim, output_dim, device=device, dtype=torch.float32)*0.01\n",
    "        self.b = torch.zeros(output_dim, device=device, dtype=torch.float32)\n",
    "         \n",
    "    # forward pass\n",
    "    def forward(self,X):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        logits = torch.matmul(X, self.W) + self.b   # shape (N, C)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    # Let's implement logsumexp trick for numerical stability\n",
    "    def softmax(self,z):\n",
    "        shifted = z - torch.max(z,dim=1, keepdims=True).values\n",
    "        exp_scores = torch.exp(shifted)\n",
    "        probs = exp_scores / torch.sum(exp_scores, dim=1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    # compute loss using cross-entropy\n",
    "    def compute_loss(self,probs,y_true):\n",
    "        \n",
    "        N = y_true.shape[0]\n",
    "        \n",
    "        correct_logprobs = -torch.log(probs[torch.arange(N), y_true])\n",
    "        loss = torch.sum(correct_logprobs) / N\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    # Prediction function\n",
    "    def predict(self,X):   \n",
    "        logits = self.forward(X)\n",
    "        probs = self.softmax(logits)\n",
    "        return torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        \n",
    "        \n",
    "    def backward(self, X, y_true):\n",
    "        \n",
    "        y_true = torch.tensor(y_true, dtype=torch.long, device=self.device)\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        logits = self.forward(X)\n",
    "        probs = self.softmax(logits)     # torch softmax\n",
    "\n",
    "        # One-hot encode\n",
    "        y_onehot = torch.zeros_like(probs)\n",
    "        y_onehot[torch.arange(N), y_true] = 1\n",
    "\n",
    "        dL_dlogits = (probs - y_onehot) / N\n",
    "\n",
    "        dL_dW = X.T @ dL_dlogits\n",
    "        dL_db = torch.sum(dL_dlogits, axis=0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            self.W -= self.learning_rate * dL_dW\n",
    "            self.b -= self.learning_rate * dL_db\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ff34f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, n_informative=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "439d6cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights :  tensor([[-0.0048, -0.0203, -0.0056],\n",
      "        [-0.0046, -0.0006,  0.0080],\n",
      "        [ 0.0024,  0.0024,  0.0053],\n",
      "        [-0.0072, -0.0137,  0.0253],\n",
      "        [ 0.0028,  0.0067,  0.0003],\n",
      "        [-0.0048,  0.0013,  0.0048],\n",
      "        [ 0.0007,  0.0055,  0.0036],\n",
      "        [-0.0012, -0.0014, -0.0126],\n",
      "        [ 0.0109, -0.0060, -0.0065],\n",
      "        [ 0.0129, -0.0060, -0.0170]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42795/1298338543.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "/tmp/ipykernel_42795/1298338543.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(y_true, dtype=torch.long, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000, Loss: 1.0738\n",
      "Epoch 200/10000, Loss: 1.0566\n",
      "Epoch 300/10000, Loss: 1.0425\n",
      "Epoch 400/10000, Loss: 1.0307\n",
      "Epoch 500/10000, Loss: 1.0206\n",
      "Epoch 600/10000, Loss: 1.0119\n",
      "Epoch 700/10000, Loss: 1.0043\n",
      "Epoch 800/10000, Loss: 0.9976\n",
      "Epoch 900/10000, Loss: 0.9917\n",
      "Epoch 1000/10000, Loss: 0.9863\n",
      "Epoch 1100/10000, Loss: 0.9815\n",
      "Epoch 1200/10000, Loss: 0.9772\n",
      "Epoch 1300/10000, Loss: 0.9733\n",
      "Epoch 1400/10000, Loss: 0.9696\n",
      "Epoch 1500/10000, Loss: 0.9663\n",
      "Epoch 1600/10000, Loss: 0.9633\n",
      "Epoch 1700/10000, Loss: 0.9605\n",
      "Epoch 1800/10000, Loss: 0.9578\n",
      "Epoch 1900/10000, Loss: 0.9554\n",
      "Epoch 2000/10000, Loss: 0.9531\n",
      "Epoch 2100/10000, Loss: 0.9510\n",
      "Epoch 2200/10000, Loss: 0.9490\n",
      "Epoch 2300/10000, Loss: 0.9472\n",
      "Epoch 2400/10000, Loss: 0.9454\n",
      "Epoch 2500/10000, Loss: 0.9437\n",
      "Epoch 2600/10000, Loss: 0.9422\n",
      "Epoch 2700/10000, Loss: 0.9407\n",
      "Epoch 2800/10000, Loss: 0.9393\n",
      "Epoch 2900/10000, Loss: 0.9380\n",
      "Epoch 3000/10000, Loss: 0.9367\n",
      "Epoch 3100/10000, Loss: 0.9355\n",
      "Epoch 3200/10000, Loss: 0.9344\n",
      "Epoch 3300/10000, Loss: 0.9333\n",
      "Epoch 3400/10000, Loss: 0.9322\n",
      "Epoch 3500/10000, Loss: 0.9312\n",
      "Epoch 3600/10000, Loss: 0.9303\n",
      "Epoch 3700/10000, Loss: 0.9294\n",
      "Epoch 3800/10000, Loss: 0.9285\n",
      "Epoch 3900/10000, Loss: 0.9277\n",
      "Epoch 4000/10000, Loss: 0.9268\n",
      "Epoch 4100/10000, Loss: 0.9261\n",
      "Epoch 4200/10000, Loss: 0.9253\n",
      "Epoch 4300/10000, Loss: 0.9246\n",
      "Epoch 4400/10000, Loss: 0.9239\n",
      "Epoch 4500/10000, Loss: 0.9233\n",
      "Epoch 4600/10000, Loss: 0.9226\n",
      "Epoch 4700/10000, Loss: 0.9220\n",
      "Epoch 4800/10000, Loss: 0.9214\n",
      "Epoch 4900/10000, Loss: 0.9208\n",
      "Epoch 5000/10000, Loss: 0.9203\n",
      "Epoch 5100/10000, Loss: 0.9197\n",
      "Epoch 5200/10000, Loss: 0.9192\n",
      "Epoch 5300/10000, Loss: 0.9187\n",
      "Epoch 5400/10000, Loss: 0.9182\n",
      "Epoch 5500/10000, Loss: 0.9177\n",
      "Epoch 5600/10000, Loss: 0.9173\n",
      "Epoch 5700/10000, Loss: 0.9168\n",
      "Epoch 5800/10000, Loss: 0.9164\n",
      "Epoch 5900/10000, Loss: 0.9159\n",
      "Epoch 6000/10000, Loss: 0.9155\n",
      "Epoch 6100/10000, Loss: 0.9151\n",
      "Epoch 6200/10000, Loss: 0.9148\n",
      "Epoch 6300/10000, Loss: 0.9144\n",
      "Epoch 6400/10000, Loss: 0.9140\n",
      "Epoch 6500/10000, Loss: 0.9136\n",
      "Epoch 6600/10000, Loss: 0.9133\n",
      "Epoch 6700/10000, Loss: 0.9130\n",
      "Epoch 6800/10000, Loss: 0.9126\n",
      "Epoch 6900/10000, Loss: 0.9123\n",
      "Epoch 7000/10000, Loss: 0.9120\n",
      "Epoch 7100/10000, Loss: 0.9117\n",
      "Epoch 7200/10000, Loss: 0.9114\n",
      "Epoch 7300/10000, Loss: 0.9111\n",
      "Epoch 7400/10000, Loss: 0.9108\n",
      "Epoch 7500/10000, Loss: 0.9105\n",
      "Epoch 7600/10000, Loss: 0.9103\n",
      "Epoch 7700/10000, Loss: 0.9100\n",
      "Epoch 7800/10000, Loss: 0.9097\n",
      "Epoch 7900/10000, Loss: 0.9095\n",
      "Epoch 8000/10000, Loss: 0.9093\n",
      "Epoch 8100/10000, Loss: 0.9090\n",
      "Epoch 8200/10000, Loss: 0.9088\n",
      "Epoch 8300/10000, Loss: 0.9085\n",
      "Epoch 8400/10000, Loss: 0.9083\n",
      "Epoch 8500/10000, Loss: 0.9081\n",
      "Epoch 8600/10000, Loss: 0.9079\n",
      "Epoch 8700/10000, Loss: 0.9077\n",
      "Epoch 8800/10000, Loss: 0.9075\n",
      "Epoch 8900/10000, Loss: 0.9073\n",
      "Epoch 9000/10000, Loss: 0.9071\n",
      "Epoch 9100/10000, Loss: 0.9069\n",
      "Epoch 9200/10000, Loss: 0.9067\n",
      "Epoch 9300/10000, Loss: 0.9065\n",
      "Epoch 9400/10000, Loss: 0.9063\n",
      "Epoch 9500/10000, Loss: 0.9061\n",
      "Epoch 9600/10000, Loss: 0.9059\n",
      "Epoch 9700/10000, Loss: 0.9058\n",
      "Epoch 9800/10000, Loss: 0.9056\n",
      "Epoch 9900/10000, Loss: 0.9054\n",
      "Epoch 10000/10000, Loss: 0.9053\n",
      "tensor([[ 0.1699, -0.2188,  0.0182],\n",
      "        [ 0.0063, -0.0626,  0.0591],\n",
      "        [ 0.0836,  0.0318, -0.1052],\n",
      "        [-0.1568, -0.0150,  0.1762],\n",
      "        [-0.3465,  0.2591,  0.0972],\n",
      "        [-0.0162,  0.0609, -0.0435],\n",
      "        [ 0.0850,  0.0850, -0.1600],\n",
      "        [-0.0304,  0.0641, -0.0489],\n",
      "        [ 0.7034, -0.7626,  0.0576],\n",
      "        [-0.3551,  0.3935, -0.0486]])\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(torch.unique(y_train))\n",
    "model = Linear_Classification(input_dim, output_dim, learning_rate=0.001)\n",
    "print(\"Initial Weights : \",model.W)\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    logits = model.forward(X_train)\n",
    "    probs = model.softmax(logits)\n",
    "    loss = cross_entropy(probs, y_train)\n",
    "    model.backward(X_train, y_train)\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "print(model.W)\n",
    "# Evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05274568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

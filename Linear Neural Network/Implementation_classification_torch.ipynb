{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e322a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import cross_entropy \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim, learning_rate=0.01, device='cpu'):\n",
    "        # Initialize weights and bias\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim).to(device)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "661a9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, n_informative=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6346cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000, Loss: 1.3622\n",
      "Epoch 200/10000, Loss: 1.2477\n",
      "Epoch 300/10000, Loss: 1.1568\n",
      "Epoch 400/10000, Loss: 1.0856\n",
      "Epoch 500/10000, Loss: 1.0301\n",
      "Epoch 600/10000, Loss: 0.9867\n",
      "Epoch 700/10000, Loss: 0.9524\n",
      "Epoch 800/10000, Loss: 0.9249\n",
      "Epoch 900/10000, Loss: 0.9023\n",
      "Epoch 1000/10000, Loss: 0.8836\n",
      "Epoch 1100/10000, Loss: 0.8678\n",
      "Epoch 1200/10000, Loss: 0.8543\n",
      "Epoch 1300/10000, Loss: 0.8425\n",
      "Epoch 1400/10000, Loss: 0.8322\n",
      "Epoch 1500/10000, Loss: 0.8231\n",
      "Epoch 1600/10000, Loss: 0.8150\n",
      "Epoch 1700/10000, Loss: 0.8078\n",
      "Epoch 1800/10000, Loss: 0.8012\n",
      "Epoch 1900/10000, Loss: 0.7953\n",
      "Epoch 2000/10000, Loss: 0.7899\n",
      "Epoch 2100/10000, Loss: 0.7850\n",
      "Epoch 2200/10000, Loss: 0.7805\n",
      "Epoch 2300/10000, Loss: 0.7763\n",
      "Epoch 2400/10000, Loss: 0.7725\n",
      "Epoch 2500/10000, Loss: 0.7690\n",
      "Epoch 2600/10000, Loss: 0.7657\n",
      "Epoch 2700/10000, Loss: 0.7627\n",
      "Epoch 2800/10000, Loss: 0.7599\n",
      "Epoch 2900/10000, Loss: 0.7572\n",
      "Epoch 3000/10000, Loss: 0.7548\n",
      "Epoch 3100/10000, Loss: 0.7524\n",
      "Epoch 3200/10000, Loss: 0.7503\n",
      "Epoch 3300/10000, Loss: 0.7483\n",
      "Epoch 3400/10000, Loss: 0.7464\n",
      "Epoch 3500/10000, Loss: 0.7446\n",
      "Epoch 3600/10000, Loss: 0.7429\n",
      "Epoch 3700/10000, Loss: 0.7413\n",
      "Epoch 3800/10000, Loss: 0.7397\n",
      "Epoch 3900/10000, Loss: 0.7383\n",
      "Epoch 4000/10000, Loss: 0.7370\n",
      "Epoch 4100/10000, Loss: 0.7357\n",
      "Epoch 4200/10000, Loss: 0.7345\n",
      "Epoch 4300/10000, Loss: 0.7333\n",
      "Epoch 4400/10000, Loss: 0.7322\n",
      "Epoch 4500/10000, Loss: 0.7311\n",
      "Epoch 4600/10000, Loss: 0.7301\n",
      "Epoch 4700/10000, Loss: 0.7292\n",
      "Epoch 4800/10000, Loss: 0.7283\n",
      "Epoch 4900/10000, Loss: 0.7274\n",
      "Epoch 5000/10000, Loss: 0.7266\n",
      "Epoch 5100/10000, Loss: 0.7258\n",
      "Epoch 5200/10000, Loss: 0.7250\n",
      "Epoch 5300/10000, Loss: 0.7243\n",
      "Epoch 5400/10000, Loss: 0.7236\n",
      "Epoch 5500/10000, Loss: 0.7229\n",
      "Epoch 5600/10000, Loss: 0.7223\n",
      "Epoch 5700/10000, Loss: 0.7217\n",
      "Epoch 5800/10000, Loss: 0.7211\n",
      "Epoch 5900/10000, Loss: 0.7205\n",
      "Epoch 6000/10000, Loss: 0.7199\n",
      "Epoch 6100/10000, Loss: 0.7194\n",
      "Epoch 6200/10000, Loss: 0.7189\n",
      "Epoch 6300/10000, Loss: 0.7184\n",
      "Epoch 6400/10000, Loss: 0.7179\n",
      "Epoch 6500/10000, Loss: 0.7175\n",
      "Epoch 6600/10000, Loss: 0.7170\n",
      "Epoch 6700/10000, Loss: 0.7166\n",
      "Epoch 6800/10000, Loss: 0.7162\n",
      "Epoch 6900/10000, Loss: 0.7158\n",
      "Epoch 7000/10000, Loss: 0.7154\n",
      "Epoch 7100/10000, Loss: 0.7150\n",
      "Epoch 7200/10000, Loss: 0.7147\n",
      "Epoch 7300/10000, Loss: 0.7143\n",
      "Epoch 7400/10000, Loss: 0.7140\n",
      "Epoch 7500/10000, Loss: 0.7136\n",
      "Epoch 7600/10000, Loss: 0.7133\n",
      "Epoch 7700/10000, Loss: 0.7130\n",
      "Epoch 7800/10000, Loss: 0.7127\n",
      "Epoch 7900/10000, Loss: 0.7124\n",
      "Epoch 8000/10000, Loss: 0.7121\n",
      "Epoch 8100/10000, Loss: 0.7118\n",
      "Epoch 8200/10000, Loss: 0.7116\n",
      "Epoch 8300/10000, Loss: 0.7113\n",
      "Epoch 8400/10000, Loss: 0.7110\n",
      "Epoch 8500/10000, Loss: 0.7108\n",
      "Epoch 8600/10000, Loss: 0.7106\n",
      "Epoch 8700/10000, Loss: 0.7103\n",
      "Epoch 8800/10000, Loss: 0.7101\n",
      "Epoch 8900/10000, Loss: 0.7099\n",
      "Epoch 9000/10000, Loss: 0.7096\n",
      "Epoch 9100/10000, Loss: 0.7094\n",
      "Epoch 9200/10000, Loss: 0.7092\n",
      "Epoch 9300/10000, Loss: 0.7090\n",
      "Epoch 9400/10000, Loss: 0.7088\n",
      "Epoch 9500/10000, Loss: 0.7086\n",
      "Epoch 9600/10000, Loss: 0.7084\n",
      "Epoch 9700/10000, Loss: 0.7082\n",
      "Epoch 9800/10000, Loss: 0.7081\n",
      "Epoch 9900/10000, Loss: 0.7079\n",
      "Epoch 10000/10000, Loss: 0.7077\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(torch.unique(y_train))\n",
    "model = LinearClassifier(input_dim, output_dim)\n",
    "# print(\"Initial Weights : \",model.W)\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    logits = model.forward(X_train)\n",
    "   \n",
    "    loss = criterion(logits, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "# print(model.W)\n",
    "# Evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d5f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
